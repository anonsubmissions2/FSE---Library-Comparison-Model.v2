
% \section{Background and Related Work}
% %\input{tables/related-works-summary.tex} % AB: Save this for the tool paper

% We first describe the buyer behavior models which we used as the lens for viewing software library adoption. Next, we review two strands of literature which relate to our research questions. 



% \subsection{Background}
% \subsubsection{Buyer Behavior Models}
% We use buyer behavior as the lens through which we examine software library adoption decisions.
% In contrast to technology adoption models, consumer and business buyer behavior model drawn from marketing theories provide more in-depth insights into how a consumer or a business organization makes a decision to procure a product. In their seminal textbook, Kotler and Armstrong defined consumer/organization specific concerns as `influences' and  separated these from product-specific attributes (which we will henceforth refer to as factors, consistent with the software engineering literature) while explaining the influences in buying process \cite{kotler2014principles}. Moreover the product-specific factors are also a foundational element of the Fishbein Multiattribute Model which calculates the weighted average of all product-specific factors to define which product the consumer will select \cite{fishbein1967attitude}. Since it was introduced almost 60 years ago, this model has been ``extensively used by consumer researchers'' \cite{blackwell2001consumer}. %The segregation between product (here library) attributes and decision influences was established in marketing theory because of the elaborate analysis of the buying decision process. 
% In addition to influential conditions, and important product factors, the buyer behavior models also provided steps of buying decision process and actors involved or influencing the product adoption. Though these models provide more holistic approach compared to technology adoption models, there is no study how such models are applicable to technology adoption specifically, library adoption process.

% By contrast, research on library selection has focused almost entirely on technological factors, and has not looked holistically at the entire library selection process while distinguishing product-specific factors from influences.
\section{Related Work}
  In section~\ref{sec:tech-adoption}, we look at the steps of technology adoption, which relates to our first research question. In \sec\ref{sec:lit:processes}, we approach our second and third research questions by discussing the factors used in evaluation and the formal processes which have been developed to facilitate the decision, usually at an organizational level. 
  %Section~\ref{sec:lit:automation} covers efforts that have been made to automate gathering information to facilitate software selection decisions.
\subsection{Steps of Technology Adoption} %% Background for RQ1
\label{sec:tech-adoption}
The process by which a tool comes to be adopted within an open source software community was found to consist of several phases: knowledge, individual persuasion, individual decision, individual implementation, organizational adaptation, and organizational acceptance \cite{krafft:2016:free}. In the knowledge phase, for example, sedimentation describes a technology gaining recognition to the point that it is considered ready for adoption. Marketing, or the active promotion of the tool or technique to generate excitement, simulates developers to evaluate the project's utility. The final knowledge phase, peercolation, explains how information spreads between peers and how this information from trusted sources leads to favored treatment of the technology.


% Considering all related works, there has been a vacuum and necessity for understanding the complete process and principles of library adoption (not only selection, rather than maintenance as well) and its process  which is performed in this study. 

In marketing, psychology, and technology literature, a number of technology adoption models have been proposed. Fishbein and Azjen's Theory of Reasoned Action (TRA) \cite{flanders1975belief-tra} explaining consumer's belief, attitude, intention, and behavior has become foundational base for investigating personal technology usage \cite{taherdoost2018-adoption-models}. Two notable derivation of TRA model are Theory of Planned Behavior (TPB) \cite{ajzen1991-tpb} which added perceived behavioral control and Technology Acceptance Model (TAM) \cite{davis1985tam, davis1989-tam-usefulness} which considered perceived usefulness, perceived ease of use, and attitude toward use for technology adoption by individual consumers. While these models explained consumer behavior to adopt or accept a technology product, TOE Framework by Tornatzky et al. \cite{tornatzky1990processes-toe} describes how the adopting and implementing technology in organizations is influenced by Technological, Organizational, and Environmental (TOE) conditions. Whereas TOE framework provides insights how organizational adoptions could be influenced, this framework has been criticized to be too generic \cite{zhu2005post-toe-critic} and not sufficient enough for explaining software library adoption process.



\subsection{Technology Selection Factors and Processes}\label{sec:lit:processes}

Perhaps the most obvious factors involved in the selection of libraries relate the the needs of developers and the utility offered by the library. Historic factors which limited software reuse were limitations in the capacity of retrieval technologies to return relevant results and the difficulty of evaluating components \cite{hummel2008code}. Retrieval is no longer a serious concern, as modern software ecosystems have developed that make it easy for developers to incorporate libraries with searches built in to IDEs. However, the evaluation of components remains difficult, especially when the process can be hidden to the extent that malicious code can be hidden in dependency chains of popular packages \cite{wyss2022wolf}. Developers seek out API reviews in order to discover information related to their specific development needs and to compensate for shortcomings in the official documentation \cite{uddin2019understanding}. Liu et al. created a taxonomy of the API elements required by developers based on an analysis of Stack Overflow posts, identifying categories such as non-functional improvement, error handling, and API usage learning \cite{liu2021api}. Developers make use of a ``combination of code examples and opinions about an API as a form of documentation,'' relying on the positive and negative views in order to assess the quality of the API \cite{uddin2019understanding}. This is complicated by concerns about the trustworthiness, relevance, and recency of the opinions. % Also uddin2019understanding

While the aforementioned studies have focused largely on technical aspects of library selection, Larios-Vargas et al. conducted the most comprehensive study of factors associated with library selection, identifying 26 through semi-structured interviews, which were subsequently validated through a survey of 116 developers \cite{larios2020selecting}. The factors were categorized as technical, human, and economic. In contrast to our study, they did not investigate the conditions under which each factor is relevant in the selection process. Furthermore, the study did not draw a clear distinction between factors that are directly related to the libraries, and those which stem from the external environment, such as company culture, company management, and type of industry. These concerns undoubtedly influence the selection process, but the lack of segregation of library-specific factors and conditions, together with the lack of guidance on the when factors should be considered limits the extent to which the study can support the decision-making process.

% Shorter version of paragraph.
Companies trying to select the most appropriate library for a task have been provided with processes - sometimes with tool support - which strive to help them compare factors based on their internal conditions, such as Qualification and Selection Open Source (QSOS) and Open Business Readiness Rating (OpenBRR) \cite{deprez2008comparing, semeteys2008method, wasserman2017osspal}. Li et al. also identified eight key evaluation factors in considering open source software applications (e.g., community and adoption, development process, etc.) \cite{li2022exploring}.  Cruz, Wieland and Ziegler proposed a process which considers the scenario for adopting, the requirements, the interpretation of collected information, and an investigation of the criteria \cite{cruz2006evaluation}. For each scenario, the authors identify the functional, technical, organisational, economical, and political requirements associated with it. While these attempts to codify the decision process do address human and technical factors, they are primarily focused on the selection of applications, not libraries.
%% Long version of paragraph below.
% There have been multiple processes proposed for companies trying to select the most appropriate open source software for a task. Although the emphasis is typically on products, libraries are occasionally mentioned and indeed, there is nothing in the processes which would rule out their use on components. Two such processes, both of which are based on a comparison of factors based on conditions, are Qualification and Selection Open Source (QSOS) and Open Business Readiness Rating (OpenBRR). \cite{deprez2008comparing}. In QSOS, the user begins with a list of projects which seem to fit the overall requirements, evaluates them according to the criteria given by QSOS (such as intrinsic durability, technical adaptability, and documentation), adjusts the importance of each criterion based on their conditions, and then makes a decision \cite{semeteys2008method,deprez2008comparing}. OpenBRR reduces a long list of projects to a short list of candidates based on the criticality of the system and context-specific criteria such as age of the project and quality of source code. OpenBRR proposes eight key considerations: licensing, standard compliance, referencable adopters, availability of support, implementation language(s), third-party reviews, books, and review by industry analysts; a tool was developed to help developers follow the process \cite{deprez2008comparing,wasserman2017osspal}. Li et al. also identify eight key evaluation factors in considering open source software applications: community and adoption, development process, economic, functionality, license, operational software characteristics, quality, and support and service \cite{li2022exploring}. Cruz, Wieland and Ziegler proposed a process which considers the scenario for adopting (the user's situation), the requirements, the interpretation of collected information, and an investigation of the criteria \cite{cruz2006evaluation}. For each scenario, the authors identify the functional, technical, organisational, economical, and political requirements associated with it; for instance, when using open source software to reduce costs, economic considerations include sustainability, protection of investment, cost reduction, and division of development costs.

A literature review on software library and package selection studies found a need to develop a framework for software evaluation and selection \cite{jadhav2009review}, and there have been numerous attempts to provide automatic support for technology evaluation.
In a study of industry requirements for governance of open source libraries, Harutyunyan, Bauer and Riehle identified that such a tool should aid in searching for libraries, selecting the best library, and estimating the cost of using the library \cite{harutyunyan:2018:understanding}. 
Existing tools, such as DiffTech, LibComp, Opiner, and POME, have focused on these first two requirements \cite{huang2018tell, wang2020difftech, wang2021difftech,de2018library, de2018empirical, el2020libcomp,uddin2019automatic, uddin2022empirical,lin2019pattern}. 
%These tools rely on a variety of data sources. 
Another approach involved displaying annotated code examples \cite{yan2022concept}. What these tools have in common is that they each focus on a subset of aspects involved in the decision, and are unable to consider developer's specific priorities. Several library-specific studies have also focused on recommendations for particular libraries \cite{moataz2020android-reco,he2021migration}, but these are naturally limited to specific technologies.



% Library selection
%\subsubsection{Automation of Technology Evaluation}\label{sec:lit:automation}

%As the majority of programmers make use of tools such as search engines, code-specific search engines, and other resources in their selection of libraries \cite{umarji2008archetypal}, it is unsurprising that there have been several attempts to aid developer selection by developing tools to automate the comparison of libraries. In a study of industry requirements for governance of open source software libraries, Harutyunyan, Bauer and Riehle identified that such a tool should aid in searching for libraries, selecting the best library, and estimating the cost of using the library \cite{harutyunyan:2018:understanding}. Existing tools have focused largely on the first two requirements. DiffTech presents a side-by-side comparison of 2,410 pairs of technologies based on mining and clustering opinions from Stack Overflow \cite{huang2018tell, wang2020difftech, wang2021difftech}. The results highlight the factors that developers consider important in the comparison of two alternatives, but the approach is specific to the technologies studied and cannot be generalized to provide advice on decisions about other technologies. LibComp uses nine metrics to suggest Java libraries to developers through an IDE plugin \cite{de2018library, de2018empirical, el2020libcomp}. These metrics are automatically derived from the library repository and include measures such as popularity, release frequency, and issue response time. These may help developers answer questions about the sustainability of the software, one of the factors Spinellis advised examining in an experience paper on open source software selection \cite{spinellis2019select}, but it does not address other possible considerations, such as the appropriateness of the technology to the task. One attempt at addressing this problem involved displaying annotated code examples of user interfaces presented side by side \cite{yan2022concept}. Opiner is similar to LibComp in that it provides users with rankings based on factors, but the factors were determined through a study of users \cite{uddin2019automatic, uddin2022empirical}. Opinion mining and sentiment analysis techniques are used to create the evaluation. Meanwhile, Lin et al. also proposed pattern based mining technique `POME' with superior performance \cite{lin2019pattern}.   

%Moving from factors primarily of interest to developers to evaluation processes aimed at organizations, Wasserman et al. developed a tool to automate part of the OpenBRR process by collecting quantitative data from a source code repository, and qualitative reviews submitted to an online portal \cite{wasserman2017osspal}. Of the 74 sub-factors Li et al. identified as related to the evaluation of open source software applications, based on 170 metrics, 40 sub-factors can be regularly obtained through source code repositories, leading the authors to conclude that information provided about projects is often insufficient for consideration \cite{li2022exploring}.

%\subsubsection{Technology Specific Library Selection} Several library specific studies have been conducted focusing on specific technology or scenarios. For example, there are library recommendation techniques for Android technology \cite{moataz2020android-reco} and for library migration scenarios \cite{he2021migration}. Since Android operating system evolves faster, library upgrade is an important issue and few studies attempted to analyze the upgrade scenarios and found that 98\% libraries with security vulnerabilities are not updated, 28\% of Android applications lag in average 16 months behind latest library version \cite{aerr2017android-update, mcDonnell2013android-update-lagging, polese2022android-integration-history}. Few research have been conducted to identify the characteristics of popular library in different programming languages such as Java, Android, and JavaScript and found that popular libraries are more well-documented and can be more unstable \cite{sujahid2023popular-javascript, lima2020popular-characteristics}. A study on data science libraries revealed that developers in data science and general software engineering have different priority of library specific factors \cite{nadi2023datascience}. Literature review on software library or package selection studies has remarked that there is a need to develop a framework for software evaluation and selection  methodology \cite{jadhav2009review}.

